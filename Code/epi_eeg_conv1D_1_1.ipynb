{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMcs6N31D6LZ",
        "outputId": "cafe497c-5b7b-4ef5-ab83-1eb7526cd978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBI6Re1V8YVf"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhlKwS3QD4mm",
        "outputId": "1e36b3a3-37a9-445f-95d6-d367efcfcdda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "filename1 = '/content/drive/My Drive/datasets/EEGs_Nigeria_transposed/control/'\n",
        "filename2 = '/content/drive/My Drive/datasets/EEGs_Nigeria_transposed/epilipsy/'\n",
        "list1 = os.listdir(filename1)\n",
        "list2 = os.listdir(filename2)\n",
        "\n",
        "row = 8\n",
        "maxlen = 30000\n",
        "data = []\n",
        "labels = []\n",
        "count = 3\n",
        "\n",
        "for name in tqdm(list1):\n",
        "    # count -= 1\n",
        "    # if count == 0:\n",
        "    #   break\n",
        "    file = os.path.join(filename1, name)\n",
        "    df = pd.read_csv(file, header=None, error_bad_lines=False)\n",
        "    # values = df.values[row - 1]\n",
        "    value_1 = df.values[row - 1]\n",
        "    value_2 = df.values[2]\n",
        "    values = value_1[:maxlen] + value_2[:maxlen]\n",
        "    # data.append(values.tolist())\n",
        "    # data.append(values[:maxlen])\n",
        "    data.append(values)\n",
        "    labels.append(0)\n",
        "\n",
        "count = 3\n",
        "for name in tqdm(list2):\n",
        "    # count -= 1\n",
        "    # if count == 0:\n",
        "    #   break\n",
        "    file = os.path.join(filename2, name)\n",
        "    df = pd.read_csv(file, header=None, error_bad_lines=False)\n",
        "    # values = df.values[row - 1]\n",
        "    value_1 = df.values[row - 1]\n",
        "    value_2 = df.values[2]\n",
        "    values = value_1[:maxlen] + value_2[:maxlen]\n",
        "    # data.append(values.tolist())\n",
        "    # data.append(values[:maxlen])\n",
        "    data.append(values)    \n",
        "    labels.append(1)\n",
        "\n",
        "data = np.asarray(data)\n",
        "data = np.asarray([np.pad(a, (0, maxlen - len(a)), 'constant', constant_values=0) for a in data])\n",
        "labels = np.asarray(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 93/93 [03:55<00:00,  2.53s/it]\n",
            "100%|██████████| 128/128 [05:10<00:00,  2.42s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEruARE13H_T",
        "outputId": "479cb29d-43ed-4995-933c-f13ff0672d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(data.shape)\n",
        "# print(data)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(221, 30000)\n",
            "(221,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3krLu5c6Gl2"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-80Gs9CHZyD",
        "outputId": "2c282b9f-bde5-45d9-b66f-9199397261c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print(type(x_train))\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(type(x_train[0]))\n",
        "print(len(x_train[0]))\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(148, 30000)\n",
            "(73, 30000)\n",
            "<class 'numpy.ndarray'>\n",
            "30000\n",
            "[7927. 7920. 7923. ... 8205. 8213. 8207.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dghN-aKuO0XB"
      },
      "source": [
        "maxlen = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaxLF_RI6qCG"
      },
      "source": [
        "# print('Pad sequences (samples x time)')\n",
        "# x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "# x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "# print(type(x_train))\n",
        "# print(x_train.shape)\n",
        "# print(x_test.shape)\n",
        "# print(type(x_train[0]))\n",
        "# print(len(x_train[0]))\n",
        "# print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIZBL3_sPCAx",
        "outputId": "6258c6c3-4101-46f5-a8dd-eebdee8f3e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(x_train[0].shape)\n",
        "print(x_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000,)\n",
            "[7927. 7920. 7923. ... 8205. 8213. 8207.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-CEKJ7h391A"
      },
      "source": [
        "# model = Sequential()\n",
        "\n",
        "batch_size = 32\n",
        "embedding_dims = 10\n",
        "filters = 100\n",
        "kernel_size = 3\n",
        "hidden_dims = 300\n",
        "epochs = 500\n",
        "max_features = 8000\n",
        "\n",
        "# # we start off with an efficient embedding layer which maps\n",
        "# # our vocab indices into embedding_dims dimensions\n",
        "# model.add(Embedding(max_features, embedding_dims, input_length=maxlen))\n",
        "# model.add(Dropout(0.2))\n",
        "\n",
        "# # we add a Convolution1D, which will learn filters\n",
        "# # word group filters of size filter_length:\n",
        "# model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
        "# # we use max pooling:\n",
        "# model.add(GlobalMaxPooling1D())\n",
        "\n",
        "# # We add a vanilla hidden layer:\n",
        "# model.add(Dense(hidden_dims))\n",
        "# model.add(Dropout(0.2))\n",
        "# model.add(Activation('relu'))\n",
        "\n",
        "# # We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "# model.add(Dense(1))\n",
        "# model.add(Activation('sigmoid'))\n",
        "\n",
        "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av7a4g6mPoUP"
      },
      "source": [
        "from __future__ import print_function\n",
        "#data preprocessing\n",
        "import pandas as pd\n",
        "#math operations\n",
        "import numpy as np\n",
        "#data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "#host-os operations\n",
        "import os\n",
        "from os import listdir, makedirs, path\n",
        "from os.path import isfile, join, basename, splitext, isfile, exists\n",
        "from pathlib import Path\n",
        "#import keras.utils as utils\n",
        "from tqdm import tqdm_notebook\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Conv1D, MaxPooling1D, Embedding, GlobalAveragePooling1D\n",
        "from keras.optimizers import adam\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.utils import plot_model\n",
        "from keras import optimizers\n",
        "# Graphic output\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr2XWqSBO03I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "56cdde89-6cba-416b-8d90-96c5d7f41059"
      },
      "source": [
        "maxlen = 30000\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=100, kernel_size=10, activation='relu',strides=5, input_shape=(maxlen, max_features)))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(filters=200, kernel_size=10, activation='relu',strides=5))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Conv1D(filters=400, kernel_size=10, activation='relu',strides=5))\n",
        "model.add(GlobalAveragePooling1D())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mean_absolute_error', optimizer= adam(lr=1e-4))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_5 (Conv1D)            (None, 5999, 100)         8000100   \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 2999, 100)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 598, 200)          200200    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 299, 200)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 58, 400)           800400    \n",
            "_________________________________________________________________\n",
            "global_average_pooling1d_2 ( (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 400)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 401       \n",
            "=================================================================\n",
            "Total params: 9,001,101\n",
            "Trainable params: 9,001,101\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3onZAaaIfJLq",
        "outputId": "46f41916-3dca-4953-a6a1-aa4cf1bdb502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(148, 30000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEohXimk7IAp"
      },
      "source": [
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUNKlEMmP8kT"
      },
      "source": [
        "slug = 'chiku'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uy_0UP0ptSo"
      },
      "source": [
        "pathModelSave = '/content/drive/My Drive/saveModels/epi_eeg_conv1D'+'_nigiria_'+str(embedding_dims)+'_'+str(filters)+'_'+str(batch_size)+'_'+str(epochs)+'_'+slug+'_.hdf5'\n",
        "pathToSaveCSV = '/content/drive/My Drive/saveModels/epi_eeg_conv1D'+'_nigiria_'+str(embedding_dims)+'_'+str(filters)+'_'+str(batch_size)+'_'+str(epochs)+'_'+slug+'_.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYHKi82aQNK8",
        "outputId": "82ae902c-57ea-4c0e-d23c-ba109c3dc3df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pathModelSave"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/saveModels/epi_eeg_conv1D_nigiria_10_100_32_500_chiku_.hdf5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wapF8cmLrSVJ",
        "outputId": "4cfdba78-17ee-4439-e2ca-08ead0bf3fc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import time\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import CSVLogger\n",
        "\n",
        "checkpoint = ModelCheckpoint(pathModelSave, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "csv_logger = CSVLogger(pathToSaveCSV, append=False, separator=',')\n",
        "\n",
        "tic = time.clock()\n",
        "history = model.fit(\n",
        "          x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          callbacks = [checkpoint, csv_logger],\n",
        "          validation_data=(x_test, y_test))\n",
        "toc = time.clock()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-d32d56822864>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m           \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv1d_5_input to have 3 dimensions, but got array with shape (148, 30000)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f9S9rFMVuIw"
      },
      "source": [
        "time = (toc-tic)/60\n",
        "print(\"Total training time is: \", time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HszbTl9V4wH"
      },
      "source": [
        "# acc> 54.795 %, tt: 1.380903566666666 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 1\n",
        "# in row 2, negative values\n",
        "# acc> 56.164 %, tt: 1.2844660333333309 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 3\n",
        "# in row 4, negative values\n",
        "# in row 5, negative values\n",
        "# acc> 49.315 %, tt: 1.2784963500000004 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 6\n",
        "# in row 7, negative values\n",
        "# acc> 56.164 %, tt: 1.331469983333333 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 8\n",
        "# acc> 35.616 %, tt: 1.2714899166666782 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 9\n",
        "# in row 10, negative values\n",
        "# in row 11, negative values\n",
        "# acc> 45.205 %, tt: 1.2967367499999995 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 12\n",
        "# acc> 45.205 %, tt: 1.2612400166666664 , ml: 400 , bs: 32 , e_d: 5 , f: 80 , k_s: 3 , h_d: 300 , epo: 500 , m_f: 10000 , of row number: 13\n",
        "# in row 14, negative values\n",
        "\n",
        "_, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('acc> %.3f' % (acc* 100.0), '%, tt:', time,', ml:',maxlen,', bs:',batch_size,', e_d:',embedding_dims,\n",
        "      ', f:',filters,', k_s:',kernel_size,', h_d:',hidden_dims,', epo:',epochs,', m_f:',max_features,', of row number:',row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB1FkxYbv_Ck"
      },
      "source": [
        "from keras.models import load_model\n",
        "model_new = load_model(pathModelSave)\n",
        "\n",
        "_, acc = model_new.evaluate(x_test, y_test, verbose=0)\n",
        "print('acc> %.3f' % (acc* 100.0), '%, tt:', time,', ml:',maxlen,', bs:',batch_size,', e_d:',embedding_dims,\n",
        "      ', f:',filters,', k_s:',kernel_size,', h_d:',hidden_dims,', epo:',epochs,', m_f:',max_features,', of row number:',row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqvQRf0YN4Kr"
      },
      "source": [
        "pathModelSave"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfMKzcNBxyYD"
      },
      "source": [
        "# author Manoj kaushik\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Loss', 'Validation Loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}